#!/usr/bin/env python3
"""
üöÄ Framework ETL - M√©thode Markova
=================================

Framework d'ing√©nierie de donn√©es pour Extract, Transform, Load (ETL)
avec connecteurs pour SGBD, Parquet, NoSQL et plus encore.

Fonctionnalit√©s :
‚Ä¢ Extraction depuis multiples sources (SQL, NoSQL, fichiers)
‚Ä¢ Transformations configurables (filtrage, agr√©gation, nettoyage)
‚Ä¢ Chargement vers diff√©rentes destinations
‚Ä¢ Pipeline configurable avec validation
‚Ä¢ Monitoring et logging int√©gr√©s

Auteur: M√©thode Markova
Niveau: 06 - Mini-projets concrets
"""

import json
import sqlite3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from typing import Dict, List, Any, Optional, Union, Callable
from datetime import datetime, timedelta
from pathlib import Path
import logging
import time
import os
import sys

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('etl_framework.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('ETL_Framework')


class DataConnector:
    """Classe de base pour tous les connecteurs de donn√©es."""
    
    def __init__(self, name: str):
        self.name = name
        self.connection = None
        self.logger = logging.getLogger(f'Connector_{name}')
    
    def connect(self) -> bool:
        """√âtablit la connexion √† la source de donn√©es."""
        raise NotImplementedError("Les sous-classes doivent impl√©menter connect()")
    
    def disconnect(self):
        """Ferme la connexion."""
        if self.connection:
            self.connection.close()
            self.connection = None
    
    def extract(self, query: str = None, **kwargs) -> pd.DataFrame:
        """Extrait les donn√©es de la source."""
        raise NotImplementedError("Les sous-classes doivent impl√©menter extract()")
    
    def load(self, data: pd.DataFrame, destination: str, **kwargs) -> bool:
        """Charge les donn√©es vers la destination."""
        raise NotImplementedError("Les sous-classes doivent impl√©menter load()")


class SQLiteConnector(DataConnector):
    """Connecteur pour bases de donn√©es SQLite."""
    
    def __init__(self, db_path: str):
        super().__init__("SQLite")
        self.db_path = db_path
    
    def connect(self) -> bool:
        """Connexion √† la base SQLite."""
        try:
            self.connection = sqlite3.connect(self.db_path)
            self.logger.info(f"Connexion SQLite √©tablie : {self.db_path}")
            return True
        except Exception as e:
            self.logger.error(f"Erreur connexion SQLite : {e}")
            return False
    
    def extract(self, query: str = None, table: str = None, **kwargs) -> pd.DataFrame:
        """Extrait des donn√©es depuis SQLite."""
        if not self.connection:
            if not self.connect():
                return pd.DataFrame()
        
        try:
            if query:
                df = pd.read_sql_query(query, self.connection)
            elif table:
                df = pd.read_sql_query(f"SELECT * FROM {table}", self.connection)
            else:
                # Liste toutes les tables
                tables = pd.read_sql_query(
                    "SELECT name FROM sqlite_master WHERE type='table'", 
                    self.connection
                )
                print("üìã Tables disponibles :", tables['name'].tolist())
                return pd.DataFrame()
            
            self.logger.info(f"Extraction r√©ussie : {len(df)} lignes")
            return df
            
        except Exception as e:
            self.logger.error(f"Erreur extraction SQLite : {e}")
            return pd.DataFrame()
    
    def load(self, data: pd.DataFrame, table_name: str, **kwargs) -> bool:
        """Charge des donn√©es vers SQLite."""
        if not self.connection:
            if not self.connect():
                return False
        
        try:
            if_exists = kwargs.get('if_exists', 'append')
            data.to_sql(table_name, self.connection, if_exists=if_exists, index=False)
            self.logger.info(f"Chargement r√©ussi : {len(data)} lignes vers {table_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"Erreur chargement SQLite : {e}")
            return False


class ParquetConnector(DataConnector):
    """Connecteur pour fichiers Parquet."""
    
    def __init__(self, base_path: str = "data"):
        super().__init__("Parquet")
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
    
    def connect(self) -> bool:
        """V√©rifie l'acc√®s au r√©pertoire."""
        try:
            if self.base_path.exists() and self.base_path.is_dir():
                self.logger.info(f"Acc√®s Parquet configur√© : {self.base_path}")
                return True
            return False
        except Exception as e:
            self.logger.error(f"Erreur acc√®s Parquet : {e}")
            return False
    
    def extract(self, file_name: str, **kwargs) -> pd.DataFrame:
        """Lit un fichier Parquet."""
        try:
            file_path = self.base_path / file_name
            if not file_path.suffix:
                file_path = file_path.with_suffix('.parquet')
            
            if not file_path.exists():
                self.logger.warning(f"Fichier non trouv√© : {file_path}")
                return pd.DataFrame()
            
            df = pd.read_parquet(file_path)
            self.logger.info(f"Lecture Parquet r√©ussie : {len(df)} lignes depuis {file_name}")
            return df
            
        except Exception as e:
            self.logger.error(f"Erreur lecture Parquet : {e}")
            return pd.DataFrame()
    
    def load(self, data: pd.DataFrame, file_name: str, **kwargs) -> bool:
        """√âcrit vers un fichier Parquet."""
        try:
            file_path = self.base_path / file_name
            if not file_path.suffix:
                file_path = file_path.with_suffix('.parquet')
            
            # Options de compression
            compression = kwargs.get('compression', 'snappy')
            
            data.to_parquet(file_path, compression=compression, index=False)
            self.logger.info(f"√âcriture Parquet r√©ussie : {len(data)} lignes vers {file_name}")
            return True
            
        except Exception as e:
            self.logger.error(f"Erreur √©criture Parquet : {e}")
            return False
    
    def list_files(self) -> List[str]:
        """Liste les fichiers Parquet disponibles."""
        try:
            return [f.name for f in self.base_path.glob("*.parquet")]
        except Exception:
            return []


class JSONConnector(DataConnector):
    """Connecteur pour fichiers JSON (simulant NoSQL)."""
    
    def __init__(self, base_path: str = "data"):
        super().__init__("JSON")
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
    
    def connect(self) -> bool:
        """V√©rifie l'acc√®s au r√©pertoire."""
        return self.base_path.exists() and self.base_path.is_dir()
    
    def extract(self, file_name: str, **kwargs) -> pd.DataFrame:
        """Lit un fichier JSON."""
        try:
            file_path = self.base_path / file_name
            if not file_path.suffix:
                file_path = file_path.with_suffix('.json')
            
            if not file_path.exists():
                return pd.DataFrame()
            
            df = pd.read_json(file_path, **kwargs)
            self.logger.info(f"Lecture JSON r√©ussie : {len(df)} lignes")
            return df
            
        except Exception as e:
            self.logger.error(f"Erreur lecture JSON : {e}")
            return pd.DataFrame()
    
    def load(self, data: pd.DataFrame, file_name: str, **kwargs) -> bool:
        """√âcrit vers un fichier JSON."""
        try:
            file_path = self.base_path / file_name
            if not file_path.suffix:
                file_path = file_path.with_suffix('.json')
            
            orient = kwargs.get('orient', 'records')
            data.to_json(file_path, orient=orient, indent=2, force_ascii=False)
            self.logger.info(f"√âcriture JSON r√©ussie : {len(data)} lignes")
            return True
            
        except Exception as e:
            self.logger.error(f"Erreur √©criture JSON : {e}")
            return False


class DataTransformer:
    """Classe pour les transformations de donn√©es."""
    
    def __init__(self):
        self.logger = logging.getLogger('DataTransformer')
    
    def filter_data(self, data: pd.DataFrame, condition: str) -> pd.DataFrame:
        """Filtre les donn√©es selon une condition."""
        try:
            filtered = data.query(condition)
            self.logger.info(f"Filtrage : {len(data)} -> {len(filtered)} lignes")
            return filtered
        except Exception as e:
            self.logger.error(f"Erreur filtrage : {e}")
            return data
    
    def aggregate_data(self, data: pd.DataFrame, group_by: List[str], 
                      agg_config: Dict[str, str]) -> pd.DataFrame:
        """Agr√®ge les donn√©es."""
        try:
            grouped = data.groupby(group_by).agg(agg_config).reset_index()
            self.logger.info(f"Agr√©gation : {len(data)} -> {len(grouped)} lignes")
            return grouped
        except Exception as e:
            self.logger.error(f"Erreur agr√©gation : {e}")
            return data
    
    def clean_data(self, data: pd.DataFrame, **options) -> pd.DataFrame:
        """Nettoie les donn√©es."""
        try:
            cleaned = data.copy()
            
            # Supprime les doublons
            if options.get('remove_duplicates', False):
                cleaned = cleaned.drop_duplicates()
            
            # G√®re les valeurs manquantes
            if options.get('fill_na'):
                cleaned = cleaned.fillna(options['fill_na'])
            elif options.get('drop_na', False):
                cleaned = cleaned.dropna()
            
            # Convertit les types
            if options.get('convert_types'):
                for col, dtype in options['convert_types'].items():
                    if col in cleaned.columns:
                        cleaned[col] = pd.to_datetime(cleaned[col]) if dtype == 'datetime' else cleaned[col].astype(dtype)
            
            self.logger.info(f"Nettoyage : {len(data)} -> {len(cleaned)} lignes")
            return cleaned
            
        except Exception as e:
            self.logger.error(f"Erreur nettoyage : {e}")
            return data
    
    def add_computed_columns(self, data: pd.DataFrame, 
                           computations: Dict[str, str]) -> pd.DataFrame:
        """Ajoute des colonnes calcul√©es."""
        try:
            result = data.copy()
            for col_name, expression in computations.items():
                result[col_name] = result.eval(expression)
            
            self.logger.info(f"Ajout de {len(computations)} colonnes calcul√©es")
            return result
            
        except Exception as e:
            self.logger.error(f"Erreur colonnes calcul√©es : {e}")
            return data


class ETLPipeline:
    """Pipeline ETL principal."""
    
    def __init__(self, name: str):
        self.name = name
        self.source_connector = None
        self.target_connector = None
        self.transformer = DataTransformer()
        self.steps = []
        self.logger = logging.getLogger(f'Pipeline_{name}')
        self.metrics = {
            'start_time': None,
            'end_time': None,
            'rows_extracted': 0,
            'rows_loaded': 0,
            'status': 'PENDING'
        }
    
    def set_source(self, connector: DataConnector):
        """D√©finit la source de donn√©es."""
        self.source_connector = connector
        return self
    
    def set_target(self, connector: DataConnector):
        """D√©finit la cible de donn√©es."""
        self.target_connector = connector
        return self
    
    def add_transformation(self, transform_func: Callable, **kwargs):
        """Ajoute une transformation au pipeline."""
        self.steps.append({
            'type': 'transform',
            'function': transform_func,
            'kwargs': kwargs
        })
        return self
    
    def add_filter(self, condition: str):
        """Ajoute un filtre au pipeline."""
        self.steps.append({
            'type': 'filter',
            'condition': condition
        })
        return self
    
    def add_aggregation(self, group_by: List[str], agg_config: Dict[str, str]):
        """Ajoute une agr√©gation au pipeline."""
        self.steps.append({
            'type': 'aggregate',
            'group_by': group_by,
            'agg_config': agg_config
        })
        return self
    
    def run(self, extract_params: Dict = None, load_params: Dict = None) -> bool:
        """Ex√©cute le pipeline ETL."""
        self.metrics['start_time'] = datetime.now()
        self.metrics['status'] = 'RUNNING'
        
        try:
            # EXTRACT
            self.logger.info(f"üöÄ D√©but du pipeline '{self.name}'")
            
            if not self.source_connector:
                raise ValueError("Aucun connecteur source d√©fini")
            
            extract_params = extract_params or {}
            data = self.source_connector.extract(**extract_params)
            
            if data.empty:
                self.logger.warning("Aucune donn√©e extraite")
                self.metrics['status'] = 'WARNING'
                return False
            
            self.metrics['rows_extracted'] = len(data)
            self.logger.info(f"‚úÖ Extraction : {len(data)} lignes")
            
            # TRANSFORM
            for step in self.steps:
                if step['type'] == 'transform':
                    data = step['function'](data, **step['kwargs'])
                elif step['type'] == 'filter':
                    data = self.transformer.filter_data(data, step['condition'])
                elif step['type'] == 'aggregate':
                    data = self.transformer.aggregate_data(
                        data, step['group_by'], step['agg_config']
                    )
            
            # LOAD
            if self.target_connector:
                load_params = load_params or {}
                success = self.target_connector.load(data, **load_params)
                
                if success:
                    self.metrics['rows_loaded'] = len(data)
                    self.metrics['status'] = 'SUCCESS'
                    self.logger.info(f"‚úÖ Chargement : {len(data)} lignes")
                else:
                    self.metrics['status'] = 'FAILED'
                    return False
            
            return True
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur pipeline : {e}")
            self.metrics['status'] = 'FAILED'
            return False
            
        finally:
            self.metrics['end_time'] = datetime.now()
            duration = (self.metrics['end_time'] - self.metrics['start_time']).total_seconds()
            self.logger.info(f"üèÅ Pipeline termin√© en {duration:.2f}s - Status: {self.metrics['status']}")
    
    def get_metrics(self) -> Dict:
        """Retourne les m√©triques d'ex√©cution."""
        metrics = self.metrics.copy()
        if metrics['start_time'] and metrics['end_time']:
            metrics['duration_seconds'] = (
                metrics['end_time'] - metrics['start_time']
            ).total_seconds()
        return metrics


class ETLFramework:
    """Framework ETL principal avec interface utilisateur."""
    
    def __init__(self):
        self.connectors = {}
        self.pipelines = {}
        self.logger = logging.getLogger('ETLFramework')
        self._init_sample_data()
    
    def _init_sample_data(self):
        """Initialise des donn√©es d'exemple."""
        # Cr√©e une base SQLite avec des donn√©es d'exemple
        db_path = "sample_data.db"
        if not Path(db_path).exists():
            conn = sqlite3.connect(db_path)
            
            # Table des ventes
            sales_data = pd.DataFrame({
                'id': range(1, 101),
                'product': ['Produit_A', 'Produit_B', 'Produit_C'] * 33 + ['Produit_A'],
                'quantity': [i % 10 + 1 for i in range(100)],
                'price': [round(10 + (i % 50) * 0.5, 2) for i in range(100)],
                'date': pd.date_range('2023-01-01', periods=100, freq='D'),
                'region': ['Nord', 'Sud', 'Est', 'Ouest'] * 25
            })
            sales_data.to_sql('sales', conn, if_exists='replace', index=False)
            
            # Table des clients
            clients_data = pd.DataFrame({
                'id': range(1, 51),
                'name': [f'Client_{i}' for i in range(1, 51)],
                'age': [20 + i % 60 for i in range(50)],
                'city': ['Paris', 'Lyon', 'Marseille', 'Toulouse', 'Nice'] * 10,
                'segment': ['Premium', 'Standard', 'Basic'] * 16 + ['Premium', 'Standard']
            })
            clients_data.to_sql('clients', conn, if_exists='replace', index=False)
            
            conn.close()
            self.logger.info("üìä Donn√©es d'exemple cr√©√©es")
    
    def register_connector(self, name: str, connector: DataConnector):
        """Enregistre un connecteur."""
        self.connectors[name] = connector
        self.logger.info(f"üì° Connecteur '{name}' enregistr√©")
    
    def create_pipeline(self, name: str) -> ETLPipeline:
        """Cr√©e un nouveau pipeline."""
        pipeline = ETLPipeline(name)
        self.pipelines[name] = pipeline
        return pipeline
    
    def run_pipeline(self, name: str, **kwargs) -> bool:
        """Ex√©cute un pipeline par nom."""
        if name not in self.pipelines:
            self.logger.error(f"Pipeline '{name}' non trouv√©")
            return False
        
        return self.pipelines[name].run(**kwargs)
    
    def list_connectors(self):
        """Affiche les connecteurs disponibles."""
        print("\nüì° CONNECTEURS DISPONIBLES :")
        print("-" * 40)
        for name, connector in self.connectors.items():
            status = "üü¢ Connect√©" if connector.connection else "üî¥ D√©connect√©"
            print(f"‚Ä¢ {name:<15} ({connector.__class__.__name__}) - {status}")
    
    def list_pipelines(self):
        """Affiche les pipelines disponibles."""
        print("\nüîÑ PIPELINES DISPONIBLES :")
        print("-" * 40)
        for name, pipeline in self.pipelines.items():
            status = pipeline.metrics['status']
            emoji = {'SUCCESS': '‚úÖ', 'FAILED': '‚ùå', 'RUNNING': 'üîÑ', 'PENDING': '‚è≥'}.get(status, '‚ùì')
            print(f"‚Ä¢ {name:<20} - {emoji} {status}")
    
    def show_pipeline_metrics(self, name: str):
        """Affiche les m√©triques d'un pipeline."""
        if name not in self.pipelines:
            print(f"‚ùå Pipeline '{name}' non trouv√©")
            return
        
        metrics = self.pipelines[name].get_metrics()
        print(f"\nüìä M√âTRIQUES DU PIPELINE '{name}' :")
        print("-" * 50)
        print(f"Status           : {metrics['status']}")
        print(f"Lignes extraites : {metrics['rows_extracted']:,}")
        print(f"Lignes charg√©es  : {metrics['rows_loaded']:,}")
        
        if 'duration_seconds' in metrics:
            print(f"Dur√©e            : {metrics['duration_seconds']:.2f}s")
        
        if metrics['start_time']:
            print(f"D√©but            : {metrics['start_time'].strftime('%H:%M:%S')}")
        if metrics['end_time']:
            print(f"Fin              : {metrics['end_time'].strftime('%H:%M:%S')}")


def clear_screen():
    """Efface l'√©cran."""
    os.system('cls' if os.name == 'nt' else 'clear')


def demo_basic_etl():
    """D√©monstration ETL basique."""
    print("\nüéØ D√âMONSTRATION ETL BASIQUE")
    print("=" * 50)
    
    # Initialise le framework
    framework = ETLFramework()
    
    # Configure les connecteurs
    sqlite_conn = SQLiteConnector("sample_data.db")
    parquet_conn = ParquetConnector("data")
    json_conn = JSONConnector("data")
    
    framework.register_connector("sqlite", sqlite_conn)
    framework.register_connector("parquet", parquet_conn)
    framework.register_connector("json", json_conn)
    
    # Pipeline 1: SQLite -> Parquet
    print("\nüîÑ Pipeline 1: Export des ventes vers Parquet")
    pipeline1 = framework.create_pipeline("sales_to_parquet")
    pipeline1.set_source(sqlite_conn).set_target(parquet_conn)
    pipeline1.add_filter("quantity > 5")
    
    success = framework.run_pipeline(
        "sales_to_parquet",
        extract_params={'table': 'sales'},
        load_params={'file_name': 'sales_filtered.parquet'}
    )
    
    if success:
        print("‚úÖ Pipeline 1 termin√© avec succ√®s")
    
    # Pipeline 2: Agr√©gation et export JSON
    print("\nüîÑ Pipeline 2: Agr√©gation des ventes par r√©gion")
    pipeline2 = framework.create_pipeline("sales_aggregation")
    pipeline2.set_source(sqlite_conn).set_target(json_conn)
    pipeline2.add_aggregation(
        group_by=['region', 'product'],
        agg_config={'quantity': 'sum', 'price': 'mean'}
    )
    
    success = framework.run_pipeline(
        "sales_aggregation",
        extract_params={'table': 'sales'},
        load_params={'file_name': 'sales_by_region.json'}
    )
    
    if success:
        print("‚úÖ Pipeline 2 termin√© avec succ√®s")
    
    # Affiche les m√©triques
    framework.show_pipeline_metrics("sales_to_parquet")
    framework.show_pipeline_metrics("sales_aggregation")
    
    input("\nüëâ Appuyez sur Entr√©e pour continuer...")


def demo_advanced_pipeline():
    """D√©monstration pipeline avanc√©."""
    print("\nüöÄ D√âMONSTRATION PIPELINE AVANC√â")
    print("=" * 50)
    
    framework = ETLFramework()
    
    # Connecteurs
    sqlite_conn = SQLiteConnector("sample_data.db")
    parquet_conn = ParquetConnector("data")
    
    framework.register_connector("sqlite", sqlite_conn)
    framework.register_connector("parquet", parquet_conn)
    
    # Pipeline complexe avec transformations personnalis√©es
    def add_revenue_analysis(data: pd.DataFrame) -> pd.DataFrame:
        """Ajoute une analyse de revenus."""
        data = data.copy()
        data['revenue'] = data['quantity'] * data['price']
        data['month'] = pd.to_datetime(data['date']).dt.month
        data['revenue_category'] = pd.cut(
            data['revenue'], 
            bins=[0, 50, 100, 200, float('inf')], 
            labels=['Faible', 'Moyen', '√âlev√©', 'Premium']
        )
        return data
    
    pipeline = framework.create_pipeline("advanced_sales_analysis")
    pipeline.set_source(sqlite_conn).set_target(parquet_conn)
    pipeline.add_transformation(add_revenue_analysis)
    pipeline.add_filter("revenue > 30")
    pipeline.add_aggregation(
        group_by=['region', 'revenue_category'],
        agg_config={'revenue': 'sum', 'quantity': 'count'}
    )
    
    success = framework.run_pipeline(
        "advanced_sales_analysis",
        extract_params={'table': 'sales'},
        load_params={'file_name': 'sales_analysis.parquet'}
    )
    
    if success:
        print("‚úÖ Pipeline avanc√© termin√© avec succ√®s")
        framework.show_pipeline_metrics("advanced_sales_analysis")
    
    input("\nüëâ Appuyez sur Entr√©e pour continuer...")


def interface_connecteurs(framework: ETLFramework):
    """Interface de gestion des connecteurs."""
    while True:
        clear_screen()
        print("üì° GESTION DES CONNECTEURS")
        print("=" * 40)
        
        framework.list_connectors()
        
        print("\nüéØ OPTIONS :")
        print("1. Ajouter connecteur SQLite")
        print("2. Ajouter connecteur Parquet")
        print("3. Ajouter connecteur JSON")
        print("4. Tester connexion")
        print("0. Retour au menu principal")
        
        choix = input("\nüëâ Votre choix : ").strip()
        
        if choix == "0":
            break
        elif choix == "1":
            nom = input("üìù Nom du connecteur : ").strip()
            db_path = input("üìÇ Chemin de la base SQLite : ").strip()
            if nom and db_path:
                conn = SQLiteConnector(db_path)
                framework.register_connector(nom, conn)
                print(f"‚úÖ Connecteur '{nom}' ajout√©")
        elif choix == "2":
            nom = input("üìù Nom du connecteur : ").strip()
            data_path = input("üìÇ R√©pertoire des donn√©es (d√©faut: data) : ").strip() or "data"
            if nom:
                conn = ParquetConnector(data_path)
                framework.register_connector(nom, conn)
                print(f"‚úÖ Connecteur '{nom}' ajout√©")
        elif choix == "3":
            nom = input("üìù Nom du connecteur : ").strip()
            data_path = input("üìÇ R√©pertoire des donn√©es (d√©faut: data) : ").strip() or "data"
            if nom:
                conn = JSONConnector(data_path)
                framework.register_connector(nom, conn)
                print(f"‚úÖ Connecteur '{nom}' ajout√©")
        elif choix == "4":
            nom = input("üì° Nom du connecteur √† tester : ").strip()
            if nom in framework.connectors:
                if framework.connectors[nom].connect():
                    print(f"‚úÖ Connexion '{nom}' r√©ussie")
                else:
                    print(f"‚ùå √âchec connexion '{nom}'")
            else:
                print(f"‚ùå Connecteur '{nom}' non trouv√©")
        
        if choix != "0":
            input("\nüëâ Appuyez sur Entr√©e pour continuer...")


def interface_pipelines(framework: ETLFramework):
    """Interface de gestion des pipelines."""
    while True:
        clear_screen()
        print("üîÑ GESTION DES PIPELINES")
        print("=" * 40)
        
        framework.list_pipelines()
        
        print("\nüéØ OPTIONS :")
        print("1. Cr√©er pipeline simple")
        print("2. Ex√©cuter pipeline")
        print("3. Voir m√©triques pipeline")
        print("4. Supprimer pipeline")
        print("0. Retour au menu principal")
        
        choix = input("\nüëâ Votre choix : ").strip()
        
        if choix == "0":
            break
        elif choix == "1":
            nom = input("üìù Nom du pipeline : ").strip()
            if nom:
                pipeline = framework.create_pipeline(nom)
                
                # Configuration source
                print("\nüì° Connecteurs disponibles :")
                for name in framework.connectors.keys():
                    print(f"‚Ä¢ {name}")
                
                source = input("üîç Connecteur source : ").strip()
                target = input("üéØ Connecteur cible : ").strip()
                
                if source in framework.connectors and target in framework.connectors:
                    pipeline.set_source(framework.connectors[source])
                    pipeline.set_target(framework.connectors[target])
                    
                    # Ajouter un filtre optionnel
                    filtre = input("üîç Filtre optionnel (ex: quantity > 5) : ").strip()
                    if filtre:
                        pipeline.add_filter(filtre)
                    
                    print(f"‚úÖ Pipeline '{nom}' cr√©√©")
                else:
                    print("‚ùå Connecteur(s) invalide(s)")
        
        elif choix == "2":
            nom = input("üöÄ Nom du pipeline √† ex√©cuter : ").strip()
            if nom in framework.pipelines:
                # Param√®tres d'extraction
                table = input("üìã Table/fichier source : ").strip()
                destination = input("üéØ Destination : ").strip()
                
                extract_params = {'table': table} if table else {}
                load_params = {'table_name': destination} if destination else {}
                
                if framework.run_pipeline(nom, extract_params=extract_params, load_params=load_params):
                    print(f"‚úÖ Pipeline '{nom}' ex√©cut√© avec succ√®s")
                else:
                    print(f"‚ùå √âchec du pipeline '{nom}'")
            else:
                print(f"‚ùå Pipeline '{nom}' non trouv√©")
        
        elif choix == "3":
            nom = input("üìä Nom du pipeline : ").strip()
            if nom in framework.pipelines:
                framework.show_pipeline_metrics(nom)
            else:
                print(f"‚ùå Pipeline '{nom}' non trouv√©")
        
        elif choix == "4":
            nom = input("üóëÔ∏è Nom du pipeline √† supprimer : ").strip()
            if nom in framework.pipelines:
                del framework.pipelines[nom]
                print(f"‚úÖ Pipeline '{nom}' supprim√©")
            else:
                print(f"‚ùå Pipeline '{nom}' non trouv√©")
        
        if choix != "0":
            input("\nüëâ Appuyez sur Entr√©e pour continuer...")


def main():
    """Interface principale du framework ETL."""
    framework = ETLFramework()
    
    # Initialise les connecteurs par d√©faut
    sqlite_conn = SQLiteConnector("sample_data.db")
    parquet_conn = ParquetConnector("data")
    json_conn = JSONConnector("data")
    
    framework.register_connector("sqlite_default", sqlite_conn)
    framework.register_connector("parquet_default", parquet_conn)
    framework.register_connector("json_default", json_conn)
    
    while True:
        clear_screen()
        print("üöÄ FRAMEWORK ETL - M√âTHODE MARKOVA")
        print("=" * 60)
        print("Framework d'ing√©nierie de donn√©es avec connecteurs multiples")
        print()
        
        print("üéØ MENU PRINCIPAL :")
        print("1. üé¨ D√©monstration ETL basique")
        print("2. üöÄ D√©monstration pipeline avanc√©")
        print("3. üì° Gestion des connecteurs")
        print("4. üîÑ Gestion des pipelines")
        print("5. üìä Voir tous les m√©triques")
        print("6. üìÅ Explorer les fichiers g√©n√©r√©s")
        print("0. üö™ Quitter")
        print("-" * 60)
        
        choix = input("üëâ Votre choix : ").strip()
        
        try:
            if choix == "0":
                clear_screen()
                print("üëã Merci d'avoir utilis√© le Framework ETL !")
                print("üöÄ Continuez √† explorer l'ing√©nierie de donn√©es !")
                break
            
            elif choix == "1":
                demo_basic_etl()
            
            elif choix == "2":
                demo_advanced_pipeline()
            
            elif choix == "3":
                interface_connecteurs(framework)
            
            elif choix == "4":
                interface_pipelines(framework)
            
            elif choix == "5":
                clear_screen()
                print("üìä M√âTRIQUES DE TOUS LES PIPELINES")
                print("=" * 50)
                for name in framework.pipelines.keys():
                    framework.show_pipeline_metrics(name)
                    print()
                input("\nüëâ Appuyez sur Entr√©e pour continuer...")
            
            elif choix == "6":
                clear_screen()
                print("üìÅ FICHIERS G√âN√âR√âS")
                print("=" * 30)
                
                data_path = Path("data")
                if data_path.exists():
                    files = list(data_path.glob("*"))
                    if files:
                        for file in files:
                            size = file.stat().st_size
                            print(f"üìÑ {file.name:<25} ({size:,} octets)")
                    else:
                        print("üì≠ Aucun fichier g√©n√©r√©")
                else:
                    print("üì≠ R√©pertoire 'data' non trouv√©")
                
                print(f"\nüóÑÔ∏è Base SQLite : sample_data.db")
                if Path("sample_data.db").exists():
                    size = Path("sample_data.db").stat().st_size
                    print(f"   Taille : {size:,} octets")
                
                input("\nüëâ Appuyez sur Entr√©e pour continuer...")
            
            else:
                print("‚ùå Choix invalide")
                input("üëâ Appuyez sur Entr√©e pour continuer...")
        
        except KeyboardInterrupt:
            print("\n\nüëã Programme interrompu !")
            break
        except Exception as e:
            print(f"‚ùå Erreur inattendue : {e}")
            input("üëâ Appuyez sur Entr√©e pour continuer...")


if __name__ == "__main__":
    # V√©rification des d√©pendances
    try:
        import pandas as pd
        import pyarrow as pa
        import pyarrow.parquet as pq
        main()
    except ImportError as e:
        print("‚ùå D√©pendance manquante :")
        print(f"   {e}")
        print("\nüí° Installez les d√©pendances avec :")
        print("   pip install pandas pyarrow") 