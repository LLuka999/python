# ğŸš€ Guide du Framework ETL - MÃ©thode Markova

## Vue d'ensemble

Le Framework ETL est un systÃ¨me complet d'ingÃ©nierie de donnÃ©es qui permet d'extraire, transformer et charger des donnÃ©es entre diffÃ©rentes sources. Il illustre les concepts fondamentaux du traitement de donnÃ©es Ã  grande Ã©chelle.

## ğŸ¯ Objectifs d'apprentissage

- **Architecture modulaire** : Comprendre la sÃ©paration des responsabilitÃ©s
- **Connecteurs de donnÃ©es** : Interfacer avec diffÃ©rentes sources (SQL, NoSQL, fichiers)
- **Pipelines ETL** : Orchestrer des flux de transformation de donnÃ©es
- **Gestion d'erreurs** : Robustesse et monitoring
- **Performance** : Optimisation et mÃ©triques

## ğŸ“‹ FonctionnalitÃ©s principales

### ğŸ”Œ Connecteurs disponibles

1. **SQLiteConnector** - Bases de donnÃ©es relationnelles
2. **ParquetConnector** - Fichiers colonnaires haute performance
3. **JSONConnector** - Documents NoSQL et APIs
4. **Architecture extensible** - Facile d'ajouter de nouveaux connecteurs

### ğŸ”„ Transformations supportÃ©es

- **Filtrage** : SÃ©lection de lignes selon des critÃ¨res
- **AgrÃ©gation** : Regroupement et calculs statistiques
- **Nettoyage** : Gestion des valeurs manquantes et doublons
- **Colonnes calculÃ©es** : Expressions et formules personnalisÃ©es
- **Transformations custom** : Fonctions Python arbitraires

### ğŸ“Š Monitoring et mÃ©triques

- Logs dÃ©taillÃ©s de chaque Ã©tape
- MÃ©triques de performance (temps, volumes)
- Statut d'exÃ©cution en temps rÃ©el
- TraÃ§abilitÃ© complÃ¨te des pipelines

## ğŸš€ Installation et dÃ©marrage

### PrÃ©requis
```bash
pip install pandas pyarrow
```

### Lancement rapide
```bash
python 07_etl_framework.py
```

## ğŸ’¡ Exemples d'utilisation

### 1. Pipeline basique : SQLite â†’ Parquet

```python
# CrÃ©er le framework
framework = ETLFramework()

# Configurer les connecteurs
sqlite_conn = SQLiteConnector("data.db")
parquet_conn = ParquetConnector("output")

# CrÃ©er le pipeline
pipeline = framework.create_pipeline("export_sales")
pipeline.set_source(sqlite_conn).set_target(parquet_conn)
pipeline.add_filter("amount > 100")

# ExÃ©cuter
framework.run_pipeline("export_sales", 
                      extract_params={'table': 'sales'},
                      load_params={'file_name': 'sales_filtered.parquet'})
```

### 2. Pipeline avec agrÃ©gation

```python
pipeline = framework.create_pipeline("monthly_summary")
pipeline.set_source(sqlite_conn).set_target(json_conn)
pipeline.add_aggregation(
    group_by=['month', 'region'],
    agg_config={'revenue': 'sum', 'transactions': 'count'}
)
```

### 3. Transformation personnalisÃ©e

```python
def calculate_profit_margin(data):
    data['profit_margin'] = (data['revenue'] - data['cost']) / data['revenue'] * 100
    return data

pipeline.add_transformation(calculate_profit_margin)
```

## ğŸ—ï¸ Architecture du systÃ¨me

### Composants principaux

1. **DataConnector** (classe abstraite)
   - Interface unifiÃ©e pour toutes les sources
   - MÃ©thodes : `connect()`, `extract()`, `load()`

2. **ETLPipeline**
   - Orchestration des Ã©tapes ETL
   - Gestion des mÃ©triques et erreurs
   - ChaÃ®nage des transformations

3. **DataTransformer**
   - BibliothÃ¨que de transformations courantes
   - Extensible avec fonctions custom

4. **ETLFramework**
   - Point d'entrÃ©e principal
   - Gestion des connecteurs et pipelines
   - Interface utilisateur

### Flux de donnÃ©es

```
Source â†’ Extract â†’ Transform â†’ Load â†’ Destination
  â†“         â†“         â†“         â†“         â†“
SQLite   Pandas    Filter    Pandas   Parquet
          DF      Aggregate   DF      /JSON
```

## ğŸ“ Concepts avancÃ©s

### 1. Gestion des erreurs

- **Retry automatique** : Nouvelles tentatives sur Ã©chec temporaire
- **Validation des donnÃ©es** : VÃ©rification de la cohÃ©rence
- **Rollback** : Annulation en cas d'erreur critique

### 2. Performance

- **Traitement par chunks** : Gestion de gros volumes
- **ParallÃ©lisation** : ExÃ©cution simultanÃ©e de pipelines
- **Cache** : RÃ©utilisation de rÃ©sultats intermÃ©diaires

### 3. Monitoring

- **Logs structurÃ©s** : Format standardisÃ© pour l'analyse
- **MÃ©triques temps rÃ©el** : Suivi de l'avancement
- **Alertes** : Notification en cas de problÃ¨me

## ğŸ”§ Extensions possibles

### Nouveaux connecteurs

```python
class PostgreSQLConnector(DataConnector):
    def __init__(self, connection_string):
        super().__init__("PostgreSQL")
        self.conn_string = connection_string
    
    def connect(self):
        # ImplÃ©mentation PostgreSQL
        pass
```

### Transformations avancÃ©es

- **Machine Learning** : IntÃ©gration scikit-learn
- **GÃ©ospatial** : Traitement de donnÃ©es GPS
- **Time Series** : Analyse temporelle

### IntÃ©grations

- **Apache Airflow** : Orchestration avancÃ©e
- **Apache Spark** : Traitement distribuÃ©
- **Cloud Storage** : AWS S3, Google Cloud Storage

## ğŸ“š Exercices pratiques

### Niveau dÃ©butant
1. CrÃ©er un pipeline simple CSV â†’ JSON
2. Ajouter un filtre sur les donnÃ©es
3. Calculer des statistiques de base

### Niveau intermÃ©diaire
1. Pipeline multi-Ã©tapes avec agrÃ©gations
2. Gestion d'erreurs personnalisÃ©e
3. Transformation de donnÃ©es temporelles

### Niveau avancÃ©
1. Pipeline temps rÃ©el avec monitoring
2. Optimisation des performances
3. IntÃ©gration de nouveaux connecteurs

## ğŸ› DÃ©pannage

### Erreurs courantes

1. **Module non trouvÃ©** : `pip install pandas pyarrow`
2. **Fichier non trouvÃ©** : VÃ©rifier les chemins
3. **MÃ©moire insuffisante** : Traiter par chunks

### Performance

- **DonnÃ©es volumineuses** : Utiliser Parquet avec compression
- **RequÃªtes lentes** : Optimiser les filtres SQL
- **Transformations coÃ»teuses** : ParallÃ©liser le traitement

## ğŸŒŸ Bonnes pratiques

1. **Validation** : Toujours valider les donnÃ©es d'entrÃ©e
2. **Documentation** : Commenter les transformations complexes
3. **Tests** : Tester avec des jeux de donnÃ©es variÃ©s
4. **Versioning** : Garder un historique des pipelines
5. **Monitoring** : Surveiller les performances en production

## ğŸ“– Ressources supplÃ©mentaires

- **Pandas Documentation** : https://pandas.pydata.org/
- **Apache Arrow/Parquet** : https://arrow.apache.org/
- **Data Engineering** : "Designing Data-Intensive Applications"
- **ETL Best Practices** : "The Data Warehouse Toolkit"

---

ğŸ¯ **Objectif** : MaÃ®triser les fondamentaux de l'ingÃ©nierie de donnÃ©es et comprendre comment construire des systÃ¨mes robustes de traitement de donnÃ©es.

Ce framework est une base solide pour apprendre les concepts ETL avant de passer Ã  des solutions industrielles comme Apache Airflow, Spark ou des services cloud. 